{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import folium\n",
    "import geopy.distance\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from IPython import embed\n",
    "import rasterio as rio\n",
    "import shutil\n",
    "import fileinput\n",
    "import json\n",
    "import csv\n",
    "from rasterio.windows import Window\n",
    "from matplotlib import pyplot\n",
    "import subprocess\n",
    "import logging\n",
    "import geeup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=1s7mEzCJQhlyzmbjCy-X3qqBUA_aN93HXV8ssSMb_YI&code_challenge_method=S256>https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=1s7mEzCJQhlyzmbjCy-X3qqBUA_aN93HXV8ssSMb_YI&code_challenge_method=S256</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you\n",
       "        should paste in the box below</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter verification code:  4/1AY0e-g47d2pop-NhJDynCRpXqIoc7khZZU6VY2PxCWkRZURPaouw1yp5z0c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging setup\n",
    "logging.basicConfig(filename='PSRFM_runner.log', level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for each satellite\n",
    "# potentially think about adding date ranges fetched from the google earth engine info pages to help debugging\n",
    "synthesized_mask_name = 'cloud_mask_pixelqa'\n",
    "ls5 = {\n",
    "    'imagecollection_id' : 'LANDSAT/LT05/C01/T1_SR',\n",
    "    'pixel_size' : 30,\n",
    "    'bands' : ['B1', 'B2', 'B3', 'B4', 'B5', 'B7'],\n",
    "    'mask_band' : ['pixel_qa'],\n",
    "    'mask_bits_clear' : 1,\n",
    "    'time_field' : 'system:time_start',\n",
    "    'ee_id_date_format' : 'YYYYMMdd',\n",
    "    'id_date_format' : '%Y%m%d',\n",
    "    'id_date_indexes': (-8, 0)\n",
    "}\n",
    "ls8 = {\n",
    "    'imagecollection_id' : 'LANDSAT/LC08/C01/T1_SR',\n",
    "    'pixel_size' : 30,\n",
    "    'bands' : ['B2', 'B3', 'B4', 'B5', 'B6', 'B7'],\n",
    "    'mask_band' : ['pixel_qa'], \n",
    "    'mask_bits_clear' : 1,\n",
    "    'time_field' : 'system:time_start',\n",
    "    'ee_id_date_format' : 'YYYYMMdd',\n",
    "    'id_date_format' : '%Y%m%d',\n",
    "    'id_date_indexes': (-8, 0)\n",
    "}\n",
    "s2 = {\n",
    "    'imagecollection_id' : 'COPERNICUS/S2_SR',\n",
    "    'pixel_size' : 20,\n",
    "    'bands' : ['B2', 'B3', 'B4', 'B8A', 'B11', 'B12'], #check up msk and qa60 msk is 20m qa60 is 60m\n",
    "    'mask_band' : ['QA60'],\n",
    "    'mask_bits_clear' : (10, 11), #https://gis.stackexchange.com/questions/333883/removing-clouds-from-sentinel-2-surface-reflectance-in-google-earth-engine\n",
    "    'time_field' : 'system:time_start',\n",
    "    'ee_id_date_format' : 'YYYYMMdd',\n",
    "    'id_date_format' : '%Y%m%d',\n",
    "    'id_date_indexes': (16, 24)\n",
    "}\n",
    "modis = {\n",
    "    'imagecollection_id' : 'MODIS/006/MCD43A4',\n",
    "    'pixel_size' : 500,\n",
    "    'bands' : ['Nadir_Reflectance_Band3', 'Nadir_Reflectance_Band4', \n",
    "               'Nadir_Reflectance_Band1', 'Nadir_Reflectance_Band2', \n",
    "               'Nadir_Reflectance_Band6', 'Nadir_Reflectance_Band7'],\n",
    "    'time_field' : 'system:time_start',\n",
    "    'id_date_format' : '%Y_%m_%d',\n",
    "    'id_date_indexes': (-10, 0)\n",
    "}\n",
    "image_sets = {\n",
    "    'ls5' : ls5,\n",
    "    'ls8' : ls8,\n",
    "    's2' : s2,\n",
    "    'modis' : modis\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = 'Sample_LS8.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date_range': ['2017-08-25', '2017-12-31'],\n",
      " 'drive_folder': 'australia_sample_ls8',\n",
      " 'east_long': 129.40735334495653,\n",
      " 'local_drive_folder_location': 'C:\\\\Users\\\\karan\\\\Google Drive',\n",
      " 'local_dst_base_path': 'C:\\\\Users\\\\karan\\\\Documents\\\\CCRS_2A_COOP\\\\PSRFM_Testing\\\\SentinelKelownaTest7',\n",
      " 'north_lat': -20.984313762127464,\n",
      " 'overlap_fraction': 1,\n",
      " 'percent_cloudy': 5,\n",
      " 'prediction_dates': ['2019-11-23', '2014-09-01'],\n",
      " 'satellite_choice': 'ls8',\n",
      " 'skip_if_reference_not_found': True,\n",
      " 'south_lat': -21.1491697862,\n",
      " 'west_long': 129.22395490745654}\n"
     ]
    }
   ],
   "source": [
    "# Opening and extracting input data for running this code\n",
    "with open(input_file_path, \"r\") as input_file:\n",
    "    input_dict = json.load(input_file)\n",
    "    pprint(input_dict)\n",
    "    west_long = input_dict['west_long']\n",
    "    east_long = input_dict['east_long']\n",
    "    north_lat = input_dict['north_lat']\n",
    "    south_lat = input_dict['south_lat']\n",
    "\n",
    "    # dates\n",
    "    date_range = input_dict['date_range']\n",
    "    prediction_dates = input_dict['prediction_dates']\n",
    "    # If reference images aren't found for a prediction date skip those tiles, if this is placed at false, the code will stop running when finding images for the tiles\n",
    "    skip_if_reference_not_found = input_dict['skip_if_reference_not_found']\n",
    "\n",
    "    # dataset selections\n",
    "    satellite_choice = input_dict['satellite_choice']\n",
    "    overlap_fraction = input_dict['overlap_fraction']\n",
    "    percent_cloudy = input_dict['percent_cloudy']\n",
    "    drive_folder = input_dict['drive_folder']\n",
    "    local_drive_folder_location = input_dict['local_drive_folder_location']\n",
    "\n",
    "    psrfm_info_dst = {\n",
    "        'local_dst_base_path' : input_dict['local_dst_base_path']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karan\\Documents\\CCRS_2A_COOP\\PSRFM_Testing\\SentinelKelownaTest7\n"
     ]
    }
   ],
   "source": [
    "# Adding fixed paths and additions to paths from inputs\n",
    "tif_paths = {\n",
    "    'fine_res_path' : f'{local_drive_folder_location}\\\\{drive_folder}_{satellite_choice}',\n",
    "    'mask_path' : f'{local_drive_folder_location}\\\\{drive_folder}_{satellite_choice}_mask',\n",
    "    'coarse_res_path' : f'{local_drive_folder_location}\\\\{drive_folder}_modis'\n",
    "}\n",
    "print((psrfm_info_dst['local_dst_base_path']))\n",
    "psrfm_info_dst['PSRFM_exe_path'] = '.\\\\PSRM_exe_params'\n",
    "psrfm_info_dst['dst_input_path'] = psrfm_info_dst['local_dst_base_path'] + '\\\\input'\n",
    "psrfm_info_dst['dst_param_path'] = psrfm_info_dst['local_dst_base_path'] + '\\\\params'\n",
    "psrfm_info_dst['dst_output_path'] = psrfm_info_dst['local_dst_base_path'] + '\\\\output'\n",
    "psrfm_info_dst['dst_output_gtif_path'] = psrfm_info_dst['dst_output_path'] + '\\\\geotiffs'\n",
    "psrfm_info_dst['dst_temp_path'] = psrfm_info_dst['local_dst_base_path'] + '\\\\temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # User inputted data\n",
    "# # seattle for rainy boi hrs\n",
    "# # west_long = -123.40369004277056 - 0.25\n",
    "# # east_long = -123.31133622196978\n",
    "# # north_lat = 47.254959278137015\n",
    "# # south_lat = 47.19573761464784 - 0.25\n",
    "\n",
    "# west_long = 129.22395490745654\n",
    "# east_long = 129.70735334495654 - 0.3\n",
    "# north_lat = -20.984313762127464 \n",
    "# south_lat = -21.39916978624304 + 0.25\n",
    "\n",
    "# # area south of kelowna\n",
    "# west_long = -118.45721879827498\n",
    "# east_long = -118.29671541082381\n",
    "# north_lat = 48.81204219953305\n",
    "# south_lat = 48.70980208174063\n",
    "\n",
    "# # straya for clear boi hrs\n",
    "# # west_long = 133.84410607205484\n",
    "# # east_long = west_long+(134.7422383962736 - west_long)/8 #temporary just to make a smaller area\n",
    "# # north_lat = -22.638743081339985\n",
    "# # south_lat = north_lat - (north_lat - (-23.52560283638598))/10 #temporary just to make a smaller area\n",
    "\n",
    "# # dates\n",
    "# # date range must be within same calendar year\n",
    "# date_range = ('2019-07-01', '2019-12-12') \n",
    "# # date_range = ('2017-08-25','2017-12-31')\n",
    "# prediction_dates = ['2019-08-30', '2019-11-23', '2014-09-01']\n",
    "# # If reference images aren't found for a prediction date skip those tiles, if this is placed at false, the code will stop running when finding images for the tiles\n",
    "# skip_if_reference_not_found = True\n",
    "\n",
    "# # dataset selections\n",
    "# satellite_choice = 's2'\n",
    "# overlap_fraction = 1 # how much of the minimum block size to add for the overlap. Default is one.\n",
    "# percent_cloudy = 5 # Recommending 5 percent\n",
    "# drive_folder = 'test6KelownaS2Aug30Nov23'\n",
    "# local_drive_folder_location = 'C:\\\\Users\\\\karan\\\\Google Drive'\n",
    "\n",
    "# # calculated baseline variables from user data\n",
    "# corner_coords = [[east_long, south_lat], [west_long, south_lat], [west_long, north_lat], [east_long, north_lat], [east_long, south_lat]]\n",
    "# block_size = math.floor(image_sets['modis']['pixel_size']/image_sets[satellite_choice]['pixel_size'])\n",
    "# # minimum tile size is 10 MODIS pixels, this following formula just accounts for how many pixels we need at minimum to have it as a multiple of block size which is a PSRFM req\n",
    "# min_tile_dim_px = block_size*10\n",
    "# min_tile_dim_km = (min_tile_dim_px * image_sets[satellite_choice]['pixel_size'])/1000\n",
    "# min_tile_dim_km, east_long, block_size, west_long, east_long, north_lat, south_lat, min_tile_dim_px\n",
    "\n",
    "# # paths for local PSRFM processing and conversions\n",
    "# tif_paths = {\n",
    "#     'fine_res_path' : f'{local_drive_folder_location}\\\\{drive_folder}_{satellite_choice}',\n",
    "#     'mask_path' : f'{local_drive_folder_location}\\\\{drive_folder}_{satellite_choice}_mask',\n",
    "#     'coarse_res_path' : f'{local_drive_folder_location}\\\\{drive_folder}_modis'\n",
    "# }\n",
    "# psrfm_info_dst = {\n",
    "#     'PSRFM_exe_path' : 'C:\\\\Users\\\\karan\\\\Documents\\\\CCRS_2A_COOP\\\\PSRFM_Wrapper\\\\PSRM_exe_params',\n",
    "#     'local_dst_base_path' : 'C:\\\\Users\\\\karan\\\\Documents\\\\CCRS_2A_COOP\\\\PSRFM_Testing\\\\SentinelKelownaTest6'\n",
    "# }\n",
    "# psrfm_info_dst['dst_input_path'] = psrfm_info_dst['local_dst_base_path'] + '\\\\input'\n",
    "# psrfm_info_dst['dst_param_path'] = psrfm_info_dst['local_dst_base_path'] + '\\\\params'\n",
    "# psrfm_info_dst['dst_output_path'] = psrfm_info_dst['local_dst_base_path'] + '\\\\output'\n",
    "# psrfm_info_dst['dst_output_gtif_path'] = psrfm_info_dst['dst_output_path'] + '\\\\geotiffs'\n",
    "# psrfm_info_dst['dst_temp_path'] = psrfm_info_dst['local_dst_base_path'] + '\\\\temp'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculated baseline variables from user data\n",
    "corner_coords = [[east_long, south_lat], \n",
    "                 [west_long, south_lat], \n",
    "                 [west_long, north_lat], \n",
    "                 [east_long, north_lat], \n",
    "                 [east_long, south_lat]]\n",
    "block_size = math.floor(\n",
    "    image_sets['modis']['pixel_size']/image_sets[satellite_choice]['pixel_size'])\n",
    "# minimum tile size is 10 MODIS pixels, this following formula just accounts for how many pixels we need at minimum to have it as a multiple of block size which is a PSRFM req\n",
    "min_tile_dim_px = block_size*10\n",
    "min_tile_dim_km = (min_tile_dim_px * image_sets[satellite_choice]['pixel_size'])/1000\n",
    "# min_tile_dim_km, east_long, block_size, west_long, east_long, north_lat, south_lat, min_tile_dim_px\n",
    "\n",
    "# Creating the geometry for tiles within the specified coordinates\n",
    "# first calculate the side lengths of the region selected\n",
    "x_dist = geopy.distance.geodesic([south_lat, east_long], [south_lat, west_long]).km\n",
    "y_dist = geopy.distance.geodesic([north_lat, east_long], [south_lat, east_long]).km\n",
    "\n",
    "# determine the number of tile segments to fully cover the region, rounded up to ensure overlap\n",
    "# need to look at this later to ensure that each tile has enough of an overlap to ensure pixels are a multiple of block size (aka cropping)\n",
    "x_tile_segments = math.floor(x_dist/min_tile_dim_km)\n",
    "y_tile_segments = math.floor(y_dist/min_tile_dim_km)\n",
    "\n",
    "# generate a list of ordered coordinates (west to east, north to south) based on the number of tiles\n",
    "# creating an overlap of approx 1 block_size pixels to ensure that each tile after cropping for PSRFM will still retain some overlap\n",
    "km_long = abs(east_long - west_long)/x_dist\n",
    "km_lat = abs(north_lat - south_lat)/y_dist\n",
    "long_overlap = ((overlap_fraction * block_size * image_sets[satellite_choice]['pixel_size'])/1000) * km_long\n",
    "lat_overlap = ((overlap_fraction * block_size * image_sets[satellite_choice]['pixel_size'])/1000) * km_lat\n",
    "\n",
    "# determining the coordinate jumps for each tile(sans overlap)\n",
    "x_coord_increment = abs(east_long - west_long)/x_tile_segments + long_overlap\n",
    "y_coord_increment = abs(north_lat - south_lat)/y_tile_segments + lat_overlap\n",
    "\n",
    "# creating the lists\n",
    "west_tile_coords = [east_long - (tile_no + 1) * x_coord_increment \n",
    "                    for tile_no in reversed(range(x_tile_segments))]\n",
    "east_tile_coords = [west_long + (tile_no + 1) * x_coord_increment \n",
    "                    for tile_no in range(x_tile_segments)]\n",
    "\n",
    "north_tile_coords = [south_lat + (tile_no + 1) * y_coord_increment \n",
    "                     for tile_no in reversed(range(y_tile_segments))]\n",
    "south_tile_coords = [north_lat - (tile_no + 1) * y_coord_increment \n",
    "                     for tile_no in range(y_tile_segments)]\n",
    "\n",
    "tiles = np.empty((x_tile_segments, y_tile_segments), ee.Geometry)\n",
    "for col in range(x_tile_segments):\n",
    "    east_coord = east_tile_coords[col]\n",
    "    west_coord = west_tile_coords[col]\n",
    "    for row in range(y_tile_segments):\n",
    "        north_coord = north_tile_coords[row]\n",
    "        south_coord = south_tile_coords[row]\n",
    "        tiles[col, row] = ee.Geometry.Rectangle(\n",
    "            [west_coord, south_coord, east_coord, north_coord])\n",
    "\n",
    "# comment out next line for debug      \n",
    "# x_dist, y_dist, min_tile_dim_km, x_coord_increment, y_coord_increment, west_tile_coords, east_tile_coords, north_tile_coords, south_tile_coords, tiles, x_tile_segments, y_tile_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to add bands\n",
    "# source: https://gis.stackexchange.com/questions/277059/cloud-mask-for-landsat8-on-google-earth-engine/277151\n",
    "# second source: https://mygeoblog.com/2019/07/25/working-with-bitmasks/\n",
    "\n",
    "def getQABitsls(image_qa, clear_bit, new_band_name):\n",
    "    pattern = 0\n",
    "#   landsat images only have one bit that needs to be masked so they're presented as integers in the ls8 bit\n",
    "    if type(clear_bit) == int:\n",
    "        pattern = pow(2, clear_bit)\n",
    "        return image_qa.addBands(image_qa.select(['pixel_qa'], [new_band_name])\\\n",
    "                                 .bitwiseAnd(pattern).rightShift(clear_bit).eq(0))\n",
    "#  sentinel-2 images have 2 bits for cloud masking\n",
    "    if type(clear_bit) == tuple:\n",
    "        pattern0 = pow(2, clear_bit[0])\n",
    "        pattern1 = pow(2, clear_bit[1])\n",
    "        return image_qa.addBands(image_qa.select(['QA60'], [new_band_name])\n",
    "                                 .bitwiseAnd(pattern0)\n",
    "                                 .bitwiseOr(\n",
    "                                     image_qa.select(['QA60'], [new_band_name])\n",
    "                                     .bitwiseAnd(pattern1))\n",
    "                                 .bitwiseOr(image_qa.select(['B2']).eq(0))\n",
    "                                 .eq(0).eq(0)) #turns any non zero values into 0's and then inverts it\n",
    "\n",
    "def getPercentageAreaInvalid(imgwithmask, mask_name, tile_geometry):\n",
    "#   retrieves the proportion of image compared to it's clipped tile that is viable to process (includes areas not covered by image as well as cloud cover)\n",
    "    cloudpercentage = ee.Number(imgwithmask.reduceRegion(reducer = ee.Reducer.mean())\n",
    "                                .get(mask_name))\n",
    "    areaoftile = tile_geometry.area()\n",
    "    areaofimage = imgwithmask.geometry().area()\n",
    "    proportionoftileinvalid = ee.Number(1).subtract(areaofimage.divide(areaoftile))\n",
    "# please look at this Richard and Detang\n",
    "    percentage_covered = cloudpercentage.add(proportionoftileinvalid)\n",
    "    return imgwithmask.set('calculated_invalid', percentage_covered)\n",
    "    \n",
    "    \n",
    "# def getPercentageClear(imgwithmask, mask_name):\n",
    "#     reducedimg = imgwithmask.reduceRegion(reducer = ee.Reducer.mean())\n",
    "#     return imgwithmask.set('calculated_cloud', reducedimg.get(mask_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(tiles[0,0].getInfo())\n",
    "# pprint(tiles[0,1].getInfo())\n",
    "# pprint(tiles[1,1].getInfo())\n",
    "# pprint(tiles[1,0].getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating arrays of the fine and coarse res imagecollections\n",
    "fine_res_tiles = np.empty((x_tile_segments, y_tile_segments), ee.ImageCollection)\n",
    "# array to track which dates of fine res images are used to get the MODIS images corresponding to the dates\n",
    "fine_res_dates = np.empty((x_tile_segments, y_tile_segments), dtype = list)\n",
    "\n",
    "for col in range(x_tile_segments):\n",
    "    for row in range(y_tile_segments):\n",
    "        #get a collection for all images within the date range, clipped to region and filtered for cloud cover\n",
    "        initial_collection = ee.ImageCollection(\n",
    "            image_sets[satellite_choice]['imagecollection_id'])\\\n",
    "                                .filterBounds(tiles[col, row])\\\n",
    "                                .filterDate(*date_range)\\\n",
    "                                .map(lambda image: image.clip(tiles[col, row]))\\\n",
    "                                .map(lambda image: \n",
    "                                     getQABitsls(\n",
    "                                         image, \n",
    "                                         image_sets[satellite_choice]['mask_bits_clear'], \n",
    "                                         synthesized_mask_name))\\\n",
    "                                .map(lambda image: \n",
    "                                     getPercentageAreaInvalid(\n",
    "                                         image.clip(tiles[col, row]), \n",
    "                                         synthesized_mask_name, \n",
    "                                         tiles[col, row]))\n",
    "        initial_collection = initial_collection.filterMetadata(\n",
    "            'calculated_invalid', 'less_than', percent_cloudy/100)\n",
    "        selected_images = []\n",
    "        fine_res_dates[col][row] = []\n",
    "        for date in prediction_dates:\n",
    "#          create a field calculating the distance of images from each prediction date, and find the lowest two for each date to use for PSRFM (having to use the ID as the system start time is when the picture was taken rather than the date, which PSRFM needs them to be different)\n",
    "            id_date_start_index = image_sets[satellite_choice]['id_date_indexes'][0]\n",
    "            id_date_end_index = image_sets[satellite_choice]['id_date_indexes'][1]\n",
    "            if id_date_end_index == 0:\n",
    "                initial_collection = initial_collection.map(\n",
    "                    lambda image: image.set(f'dateDist{date}', \n",
    "                                            ee.Date.difference(\n",
    "                                                ee.Date.parse(\n",
    "                                                    'YYYYMMdd', \n",
    "                                                    ee.String.slice(\n",
    "                                                        ee.String(image.id()), \n",
    "                                                        id_date_start_index)),\n",
    "                                                ee.Date(date), 'day')\n",
    "                                           )\n",
    "                )\n",
    "            else:\n",
    "                initial_collection = initial_collection.map(\n",
    "                    lambda image: image.set(f'dateDist{date}', \n",
    "                                            ee.Date.difference(\n",
    "                                                ee.Date.parse(\n",
    "                                                    'YYYYMMdd', \n",
    "                                                    ee.String.slice(\n",
    "                                                        ee.String(image.id()), \n",
    "                                                        id_date_start_index, \n",
    "                                                        id_date_end_index)),\n",
    "                                                ee.Date(date), 'day')\n",
    "                                           )\n",
    "                )\n",
    "\n",
    "#             pprint(initial_collection.filterMetadata(f'dateDist{date}', 'less_than', 0).sort(f'dateDist{date}', False).getInfo()['features'])\n",
    "#             initial_collection = initial_collection.map(\n",
    "#                 lambda image: image.set(f'dateDist{date}', \n",
    "#                                         ee.Number(image.get('system:time_start'))\n",
    "#                                         .subtract(ee.Date.millis(ee.Date(date)))\n",
    "#                                        )\n",
    "#             )\n",
    "#           aggregate dates then filter an imagecollection\n",
    "#             for feature in (initial_collection.filterMetadata(f'dateDist{date}', 'less_than', 0).sort(f'dateDist{date}', False).getInfo()['features']):\n",
    "#                 pprint(feature['id'])\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                first_img_before = initial_collection\\\n",
    "                    .filterMetadata(f'dateDist{date}', 'less_than', 0)\\\n",
    "                    .sort(f'dateDist{date}', False)\\\n",
    "                    .getInfo()['features'][0]\n",
    "                    \n",
    "            except IndexError:\n",
    "                if skip_if_reference_not_found:\n",
    "                    first_img_before = None\n",
    "                    logging.warning(f\"For Col {col} Row {row} on {date}, reference image before {date} not found and hence skipped\")\n",
    "                else:\n",
    "                    raise Exception(f\"Reference image before {date} not found\")\n",
    "                    \n",
    "            try:\n",
    "                first_img_after = initial_collection\\\n",
    "                    .filterMetadata(f'dateDist{date}', 'greater_than', 0)\\\n",
    "                    .sort(f'dateDist{date}', True)\\\n",
    "                    .getInfo()['features'][0]\n",
    "\n",
    "            except IndexError:\n",
    "                if skip_if_reference_not_found:\n",
    "                    first_img_after = None\n",
    "                    logging.warning(f\"For Col {col} Row {row} on {date}, reference image after {date} not found and hence skipped\")\n",
    "                else:\n",
    "                    raise Exception(f\"Reference image after {date} not found\")\n",
    "\n",
    "            \n",
    "#           if there isn't a valid pair of images don't add them anywhere\n",
    "            if first_img_before != None and first_img_after != None:\n",
    "                selected_images.append(first_img_before['id'])\n",
    "                selected_images.append(first_img_after['id'])\n",
    "\n",
    "    #           Insert the dates selected for PSRFM to track which MODIS images to extract later\n",
    "                if id_date_end_index == 0:\n",
    "                    first_image_date = first_img_before['id'][id_date_start_index:]\n",
    "                    second_image_date = first_img_after['id'][id_date_start_index:]\n",
    "                else:\n",
    "                    first_image_date = \\\n",
    "                        first_img_before['id'][id_date_start_index+1:id_date_end_index+1]\n",
    "                    second_image_date = \\\n",
    "                        first_img_after['id'][id_date_start_index+1:id_date_end_index+1]\n",
    "    #             make this a triplet\n",
    "                fine_res_dates[col][row]\\\n",
    "                    .append([date.replace('-', ''), first_image_date, second_image_date])\n",
    "#         shouldn't need this bit anymore, ignored tiles are logged\n",
    "#             else:\n",
    "# #                 append null to fine res dates\n",
    "#                 fine_res_dates[col][row].append(None)\n",
    "#note: removes duplicate images\n",
    "        selected_images = list(dict.fromkeys(selected_images))\n",
    "        fine_res_tiles[col, row] = ee.ImageCollection(selected_images)\\\n",
    "            .map(lambda image: image.clip(tiles[col, row]))\\\n",
    "            .map(lambda image: getQABitsls(\n",
    "                image, image_sets[satellite_choice]['mask_bits_clear'], synthesized_mask_name))\n",
    "\n",
    "# pprint((fine_res_tiles[0,0].getInfo()['features']))\n",
    "# for dates in fine_res_dates[0,0]:\n",
    "#     for date in dates:\n",
    "#         datetime\n",
    "#     print(date)\n",
    "#     print('pls')\n",
    "# pprint(fine_res_dates)\n",
    "# pprint((coarse_res_tiles[0,0].getInfo()['features']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "coarse_res_tiles = np.empty((x_tile_segments, y_tile_segments), ee.ImageCollection)\n",
    "for col in range(x_tile_segments):\n",
    "    for row in range(y_tile_segments):\n",
    "        collection = (ee.ImageCollection(image_sets['modis']['imagecollection_id'])\n",
    "                        .filterBounds(tiles[col, row])\n",
    "                        .filterDate(*date_range)\n",
    "                        .sort(image_sets['modis']['time_field'])\n",
    "                        .map(lambda image: image.clip(tiles[col, row])))\n",
    "        modis_name_format = collection.first()\\\n",
    "            .getInfo()['id'][:image_sets['modis']['id_date_indexes'][0]]\n",
    "        selected_modis_images = []\n",
    "        \n",
    "        for date_set in fine_res_dates[col, row]:\n",
    "            for date in date_set:\n",
    "                selected_modis_images\\\n",
    "                    .append(modis_name_format + date[:4] + '_' + date[4:6] + '_' + date[6:])\n",
    "#       ensures unique dates so no duplicates\n",
    "        selected_modis_images = list(dict.fromkeys(selected_modis_images))\n",
    "        coarse_res_tiles[col, row] = ee.ImageCollection(selected_modis_images)\\\n",
    "            .map(lambda image: image.clip(tiles[col, row]))\n",
    "pprint(selected_modis_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export imagecollection tile array function\n",
    "fine_res_filenames = []\n",
    "fine_res_mask_filenames = []\n",
    "coarse_res_filenames = []\n",
    "\n",
    "def date_to_day_of_year(date, format='%Y%m%d'):\n",
    "    date = datetime.datetime.strptime(date, format=format)\n",
    "    new_year_day = datetime.datetime(year=date.year, month=1, day=1)\n",
    "    return (date - new_year_day).days + 1\n",
    "\n",
    "\n",
    "def export_tile_array(tile_array, satellite_name):\n",
    "#   Used to keep track of tasks for polling status\n",
    "    task_list = []\n",
    "    mask_task_list = []\n",
    "    \n",
    "    folder_into = f'{drive_folder}_{satellite_name}'    \n",
    "    extract_mask = False\n",
    "    if satellite_name.lower().strip() != 'modis':\n",
    "        extract_mask = True\n",
    "    for col in range(len(tile_array)):\n",
    "        task_list.append([])\n",
    "        mask_task_list.append([])\n",
    "        for row in range(len(tile_array[col])):\n",
    "            for elem in range(tile_array[col, row].size().getInfo()):\n",
    "                elem_image = ee.Image(\n",
    "                    tile_array[col, row]\\\n",
    "                    .toList(tile_array[col, row]\\\n",
    "                            .size())\\\n",
    "                    .get(elem))\n",
    "                \n",
    "                if image_sets[satellite_name]['id_date_indexes'][1] == 0:\n",
    "                    img_date = datetime.datetime.strptime(\n",
    "                        elem_image\\\n",
    "                            .getInfo()['id'][image_sets[satellite_name]['id_date_indexes'][0]:], \n",
    "                        image_sets[satellite_name]['id_date_format'])\n",
    "                else:\n",
    "                    img_date = datetime.datetime.strptime(\n",
    "                        elem_image\\\n",
    "                            .getInfo()['id']\\\n",
    "                                [image_sets[satellite_name]['id_date_indexes'][0]+1\n",
    "                                 :image_sets[satellite_name]['id_date_indexes'][1]+1], \n",
    "                        image_sets[satellite_name]['id_date_format'])\n",
    "                first_day = datetime.datetime(year = img_date.year, month = 1, day = 1)\n",
    "                img_day = (img_date - first_day).days + 1\n",
    "                img_date_str = img_date.strftime('%d-%b-%Y')\n",
    "                img_filename = f'{satellite_name}C{col}R{row}_{img_day}_{img_date_str}'\n",
    "                task = ee.batch.Export.image.toDrive(\n",
    "                    image = elem_image.select(image_sets[satellite_name]['bands']).toInt16(),\n",
    "                    region = elem_image.getInfo()\\\n",
    "                        ['properties']['system:footprint']['coordinates'],\n",
    "                    crs = 'EPSG:32611',\n",
    "                    scale = image_sets[satellite_choice]['pixel_size'],\n",
    "                    folder = folder_into,\n",
    "                    description = img_filename)\n",
    "                \n",
    "                task.start()\n",
    "                task_list[col].append(task)\n",
    "                \n",
    "                if extract_mask:\n",
    "                    mask_filename = f'{satellite_name}C{col}R{row}_mask_{img_day}_{img_date_str}'\n",
    "                    mask_task = ee.batch.Export.image.toDrive(\n",
    "                        image = elem_image.select(synthesized_mask_name).toUint8(),\n",
    "                        region = elem_image.getInfo()\\\n",
    "                            ['properties']['system:footprint']['coordinates'],\n",
    "                        crs = 'EPSG:4326',\n",
    "                        scale = image_sets[satellite_choice]['pixel_size'],\n",
    "                        folder = f\"{folder_into}_mask\",\n",
    "                        description = mask_filename)\n",
    "                    mask_task.start()\n",
    "                    mask_task_list[col].append(mask_task)\n",
    "                    fine_res_filenames.append(img_filename)\n",
    "                    fine_res_mask_filenames.append(mask_filename)\n",
    "                else:\n",
    "                    coarse_res_filenames.append(img_filename)\n",
    "    return [task_list, mask_task_list]\n",
    "\n",
    "modis_tasks = export_tile_array(coarse_res_tiles, 'modis')\n",
    "fine_res_tasks = export_tile_array(fine_res_tiles, satellite_choice)\n",
    "fine_res_filenames = list(dict.fromkeys(fine_res_filenames))\n",
    "fine_res_mask_filenames = list(dict.fromkeys(fine_res_mask_filenames))\n",
    "coarse_res_filenames = list(dict.fromkeys(coarse_res_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # task based status checking\n",
    "\n",
    "# def check_task_status(task_tile_array)\n",
    "# # loop through task list then mask task exports\n",
    "#     for task_array in task_tile_array:\n",
    "#         for col in range(len(task_array)):\n",
    "#             for row in range(len(task_array[col])):\n",
    "#                 taskstatus = task_array[col][row].status()['state']\n",
    "#                 while taskstatus == 'READY' or taskstatus == 'RUNNING':\n",
    "#                     time.sleep(1)\n",
    "#                     taskstatus = task_array[col][row].status()['state']\n",
    "#                 logging.warning(f'{task_array[col][row].status()[\"description\"]} at (col, row): ({col}, {row}) exported with final state: {taskstatus}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for a full set of images to appear locally to begin psrfm processing\n",
    "# for now just check that all of the images exist in the folder\n",
    "\n",
    "fine_res_path = f'{local_drive_folder_location}\\\\{drive_folder}_{satellite_choice}'\n",
    "coarse_res_path = f'{local_drive_folder_location}\\\\{drive_folder}_modis'\n",
    "fine_mask_path = f'{local_drive_folder_location}\\\\{drive_folder}_{satellite_choice}_mask'\n",
    "print('started')\n",
    "while not (f'{drive_folder}_modis' in os.listdir(local_drive_folder_location)):\n",
    "    time.sleep(30)\n",
    "print('coarse res image folder exists')\n",
    "\n",
    "while not (f'{drive_folder}_{satellite_choice}' in os.listdir(f'{local_drive_folder_location}')):\n",
    "    time.sleep(30)\n",
    "print('fine res image folder exists')\n",
    "\n",
    "while not (f'{drive_folder}_{satellite_choice}_mask' in os.listdir(f'{local_drive_folder_location}')):\n",
    "    time.sleep(30)\n",
    "print('fine res mask folder exists')\n",
    "\n",
    "# wait for all the files to arrive\n",
    "while len(os.listdir(fine_mask_path)) != len(fine_res_mask_filenames):\n",
    "    time.sleep(30)\n",
    "print('all fine res mask images present')\n",
    "    \n",
    "while len(os.listdir(coarse_res_path)) != len(coarse_res_filenames):\n",
    "    time.sleep(30)\n",
    "print('all coarse res images present')\n",
    "\n",
    "while len(os.listdir(fine_res_path)) != len(fine_res_filenames):\n",
    "    time.sleep(30)\n",
    "print('all fine res images present')\n",
    "\n",
    "print('Ready for PSRFM with following files:')\n",
    "pprint(os.listdir(fine_res_path))\n",
    "pprint(os.listdir(coarse_res_path))\n",
    "pprint(os.listdir(fine_mask_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract date as datetime for conversion and param file\n",
    "def get_date_from_filename(filename, startindex = -15, endindex = -4):\n",
    "    date = datetime.datetime.strptime(filename[startindex:endindex], '%d-%b-%Y')\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring all local paths exist for PSRFM to run and for TIF files to be exported into\n",
    "if not os.path.exists(psrfm_info_dst['local_dst_base_path']):\n",
    "    os.mkdir(psrfm_info_dst['local_dst_base_path'])\n",
    "if not os.path.exists(psrfm_info_dst['dst_input_path']):\n",
    "    os.mkdir(psrfm_info_dst['dst_input_path'])\n",
    "if not os.path.exists(psrfm_info_dst['dst_param_path']):\n",
    "    os.mkdir(f'{psrfm_info_dst[\"dst_param_path\"]}')\n",
    "if not os.path.exists(f'{psrfm_info_dst[\"dst_param_path\"]}\\\\PSRFM_Main.exe'):\n",
    "    shutil.copy(f'{psrfm_info_dst[\"PSRFM_exe_path\"]}\\\\PSRFM_Main.exe', \n",
    "                psrfm_info_dst['dst_param_path'])\n",
    "if not os.path.exists(psrfm_info_dst['dst_output_path']):\n",
    "    os.mkdir(f'{psrfm_info_dst[\"dst_output_path\"]}')\n",
    "if not os.path.exists(psrfm_info_dst['dst_temp_path']):\n",
    "    os.mkdir(f'{psrfm_info_dst[\"dst_temp_path\"]}')\n",
    "# if not os.path.exists(f'{local_drive_folder_location}\\\\{drive_folder}'):\n",
    "#     os.mkdir(f'{local_drive_folder_location}\\\\{drive_folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping, converting to ENVI, and moving all the TIF files from GDrive to a local location \n",
    "# with some small band removals or other processing to make files ready for PSRFM to be run \n",
    "# on them\n",
    "filenames = []\n",
    "for path in tif_paths:\n",
    "    for filename in os.listdir(tif_paths[path]):\n",
    "        filenames.append(filename)\n",
    "        with rio.open(f'{tif_paths[path]}\\\\{filename}') as image_to_crop:\n",
    "            finalx = image_to_crop.meta[\"width\"] - image_to_crop.meta[\"width\"] % block_size\n",
    "            finaly = image_to_crop.meta[\"height\"] - image_to_crop.meta[\"height\"] % block_size\n",
    "            col_offset = (image_to_crop.width - finalx)/2\n",
    "            row_offset = (image_to_crop.height - finaly)/2\n",
    "            \n",
    "            subset_window = Window(col_offset, row_offset, finalx, finaly)\n",
    "            newargs = image_to_crop.meta.copy()\n",
    "            newargs.update({\n",
    "                'height': subset_window.height,\n",
    "                'width': subset_window.width,\n",
    "                'transform': rio.windows.transform(subset_window, image_to_crop.transform),\n",
    "                'driver': 'ENVI'\n",
    "            })\n",
    "            if newargs['count'] == 7:\n",
    "                newargs.update({'count' : 6})\n",
    "            with rio.open(f'{psrfm_info_dst[\"dst_input_path\"]}\\\\{filename[:-4]}.dat', \n",
    "                          'w', \n",
    "                          **newargs) as dst:\n",
    "                if 'ls8' in filename and 'mask' not in filename:\n",
    "                    dst.write(image_to_crop.read(indexes=[1, 2, 3, 4, 5, 6], \n",
    "                                                 window=subset_window))\n",
    "                else:\n",
    "                    dst.write(image_to_crop.read(window=subset_window))\n",
    "# filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''hey this \n",
    "is a test checking \n",
    "what happens''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the correct images to put into the param files and place all their metadata \n",
    "# into the param files\n",
    "for col in range(x_tile_segments):\n",
    "    for row in range(y_tile_segments):\n",
    "#     creating tuples of the filenames and dates corresponding to the current tile in \n",
    "#      {filename, date} format\n",
    "        fine_res_filenames_dates = [\n",
    "            (filename, get_date_from_filename(filename)) \n",
    "            for filename in os.listdir(psrfm_info_dst['dst_input_path']) \n",
    "            if filename.startswith(f'{satellite_choice}C{col}R{row}') \n",
    "            and not filename.startswith(f'{satellite_choice}C{col}R{row}_mask') \n",
    "            and filename.endswith('.dat')]\n",
    "        mask_filenames_dates = [\n",
    "            (filename, get_date_from_filename(filename)) \n",
    "            for filename in os.listdir(psrfm_info_dst['dst_input_path']) \n",
    "            if filename.startswith(f'{satellite_choice}C{col}R{row}_mask') \n",
    "            and filename.endswith('.dat')]\n",
    "        coarse_res_filenames_dates = [\n",
    "            (filename, get_date_from_filename(filename)) \n",
    "            for filename in os.listdir(psrfm_info_dst['dst_input_path']) \n",
    "            if filename.startswith(f'modisC{col}R{row}') \n",
    "            and filename.endswith('.dat')]\n",
    "#     sorting the coarse and fine resolution files by date (second element in tuple) \n",
    "#      to help create \"sets\" of images for PSRFM processing\n",
    "        coarse_res_filenames_dates = sorted(coarse_res_filenames_dates, key = lambda x: x[1])\n",
    "        fine_res_filenames_dates = sorted(fine_res_filenames_dates, key = lambda x: x[1])\n",
    "        mask_filenames_dates = sorted(mask_filenames_dates, key = lambda x: x[1])\n",
    "\n",
    "#     finding the indexes in coarse res images where both a fine res and coarse res \n",
    "#      image exist, which will then create pairs of start and end dates, and any coarse \n",
    "#      images inbetween will be used for predictions\n",
    "        ref_coarse_indexes = []\n",
    "        for name_date_pair in fine_res_filenames_dates:\n",
    "            matching_date_index = [\n",
    "                coarse_res_filenames_dates.index(tupl) \n",
    "                for tupl in coarse_res_filenames_dates \n",
    "                if tupl[1] == name_date_pair[1]]\n",
    "            ref_coarse_indexes.append(matching_date_index[0])\n",
    "\n",
    "#      creating \"sets\" for each start and end date pairing that's ready for PSRFM\n",
    "        PSRFM_sets = []\n",
    "        for ref_img_index in range(len(ref_coarse_indexes)-1):\n",
    "#     check whether a coarse res prediction image exists inbetween ref date 1-2, 2-3, 3-4...\n",
    "            coarse_res_set = coarse_res_filenames_dates[\n",
    "                ref_coarse_indexes[ref_img_index]:ref_coarse_indexes[ref_img_index + 1] + 1]\n",
    "#             print(ref_img_index)\n",
    "#         coarse res set should only trigger a PSRFM if there is a prediction date, rather \n",
    "#          than just start and end dates, so only append if it has that criteria aka len \n",
    "#          more than 2\n",
    "            if len(coarse_res_set) > 2:\n",
    "#             the first coarse res image is a start date, and the last is end date, find \n",
    "#              the two fine res images which correspond.\n",
    "                start_date = coarse_res_set[0][1]\n",
    "                end_date = coarse_res_set[-1][1]\n",
    "                fine_res_set = [\n",
    "                    name_date_pair \n",
    "                    for name_date_pair in fine_res_filenames_dates \n",
    "                    if name_date_pair[1] == start_date \n",
    "                    or name_date_pair[1] == end_date]\n",
    "                mask_set = [\n",
    "                    name_date_pair \n",
    "                    for name_date_pair in mask_filenames_dates \n",
    "                    if name_date_pair[1] == start_date \n",
    "                    or name_date_pair[1] == end_date]\n",
    "#             appending all necessary information for a PSRFM set to create a valid param file\n",
    "                PSRFM_sets.append({\n",
    "                    'coarse_res_filenames' : coarse_res_set,\n",
    "                    'fine_res_filenames': fine_res_set,\n",
    "                    'mask_filenames' : mask_set\n",
    "                })\n",
    "#         print(PSRFM_sets)\n",
    "#     adding prediction date PSRFM runs\n",
    "        PSRFM_ref_date_sets = []\n",
    "        for pred_img_index in range(len(ref_coarse_indexes) - 2):\n",
    "            coarse_res_set = [\n",
    "                coarse_res_filenames_dates[ref_coarse_indexes[pred_img_index]], \n",
    "                coarse_res_filenames_dates[ref_coarse_indexes[pred_img_index + 1]], \n",
    "                coarse_res_filenames_dates[ref_coarse_indexes[pred_img_index + 2]]]\n",
    "            start_date = coarse_res_set[0][1]\n",
    "            end_date = coarse_res_set[-1][1]\n",
    "            fine_res_set = [\n",
    "                name_date_pair \n",
    "                for name_date_pair in fine_res_filenames_dates \n",
    "                if name_date_pair[1] == start_date \n",
    "                or name_date_pair[1] == end_date]\n",
    "            mask_set = [\n",
    "                name_date_pair \n",
    "                for name_date_pair in mask_filenames_dates \n",
    "                if name_date_pair[1] == start_date \n",
    "                or name_date_pair[1] == end_date]\n",
    "            PSRFM_ref_date_sets.append({\n",
    "                    'coarse_res_filenames' : coarse_res_set,\n",
    "                    'fine_res_filenames': fine_res_set,\n",
    "                    'mask_filenames' : mask_set  \n",
    "            })\n",
    "#         pprint(PSRFM_ref_date_sets)\n",
    "        all_PSRFM_sets = [PSRFM_sets, PSRFM_ref_date_sets]\n",
    "        for set_idx in range(len(all_PSRFM_sets)):\n",
    "            reference_precursor = \"\"\n",
    "            if set_idx == 1:\n",
    "                reference_precursor = 'reference'\n",
    "            PSRFM_instances = len(all_PSRFM_sets[set_idx])\n",
    "            for instance in (range(PSRFM_instances)):\n",
    "    #             creating an output folder for each each instance\n",
    "                ouptut_dir = (f'{psrfm_info_dst[\"dst_output_path\"]}\n",
    "                    \\\\{reference_precursor}instance_{instance+1}')\n",
    "                if not os.path.exists(ouptut_dir):\n",
    "                    os.mkdir(ouptut_dir)\n",
    "    #             creating the parameter file for the current tile and instance of PSRFM\n",
    "                if not os.path.exists(f'{psrfm_info_dst[\"dst_param_path\"]}\\\\{col}_{row}_params{reference_precursor}_{instance + 1}.txt'):\n",
    "                    shutil.copy(f'{psrfm_info_dst[\"PSRFM_exe_path\"]}\\\\psrfm_template.txt', \n",
    "                                f'''{psrfm_info_dst[\"dst_param_path\"]}\\\\{col}_{row}_params{reference_precursor}_{instance + 1}.txt''')\n",
    "    #             now that file exists, insert the parameters\n",
    "                with rio.open(f'''{psrfm_info_dst[\"dst_input_path\"]}\\\\{all_PSRFM_sets[set_idx][instance-1][\"fine_res_filename\"][0][0]}''') \n",
    "                        as img:\n",
    "                    nrows = img.meta['height']\n",
    "                    ncols = img.meta['width']\n",
    "                    nbands = img.meta['count']\n",
    "                with fileinput.FileInput(\n",
    "                    f'''{psrfm_info_dst[\"dst_param_path\"]}\\\\{col}_{row}_params{reference_precursor}_{instance + 1}.txt''',\n",
    "                    inplace = True) as paramfile:\n",
    "                    for line in paramfile:\n",
    "                        if line.strip().startswith('IN_PAIR_COARSE_FNAME'):\n",
    "                            print(f'''IN_PAIR_COARSE_FNAME = {psrfm_info_dst[\"dst_input_path\"]}\\\\{all_PSRFM_sets[set_idx][instance-1][\"coarse_res_filenames\"][0][0]} {psrfm_info_dst[\"dst_input_path\"]}\\\\{all_PSRFM_sets[set_idx][instance-1][\"coarse_res_filenames\"][-1][0]}''')\n",
    "                        elif line.strip().startswith('IN_PAIR_FINE_FNAME'):\n",
    "                            print(f'''IN_PAIR_FINE_FNAME = {psrfm_info_dst[\"dst_input_path\"]}\\\\{all_PSRFM_sets[set_idx][instance-1][\"fine_res_filenames\"][0][0]} {psrfm_info_dst[\"dst_input_path\"]}\\\\{all_PSRFM_sets[set_idx][instance-1][\"fine_res_filenames\"][1][0]}''')                                            \n",
    "                        elif line.strip().startswith('IN_PAIR_FINE_MASK_FNAME'):\n",
    "                            print(f'''IN_PAIR_FINE_MASK_FNAME = {psrfm_info_dst[\"dst_input_path\"]}\\\\{all_PSRFM_sets[set_idx][instance-1][\"mask_filenames\"][0][0]} {psrfm_info_dst[\"dst_input_path\"]}\\\\{all_PSRFM_sets[set_idx][instance-1][\"mask_filenames\"][1][0]}''')                                                     \n",
    "                        elif line.strip().startswith('IN_PDAY_COARSE_NO'):\n",
    "                            prediction_filenames = ''\n",
    "                            for pair in all_PSRFM_sets[set_idx][instance-1]['coarse_res_filenames'][1:-1]:\n",
    "                                prediction_filenames += f''' {psrfm_info_dst[\"dst_input_path\"]}\\\\{pair[0]} \\n'''\n",
    "                            print(f\"IN_PDAY_COARSE_NO = {len(all_PSRFM_sets[set_idx][instance-1]['coarse_res_filenames']) - 2} \\n {prediction_filenames}\")\n",
    "                        elif line.strip().startswith('OUT_PREDICTION_DIR'):\n",
    "                            print(f\"OUT_PREDICTION_DIR = {ouptut_dir}\")\n",
    "                        elif line.strip().startswith('OUT_TEMP_DIR'):\n",
    "                            print(f\"OUT_TEMP_DIR = {psrfm_info_dst['dst_temp_path']}\")\n",
    "                        elif line.strip().startswith('OUT_ENVI_HDR'):\n",
    "                            print(f\"OUT_ENVI_HDR = {psrfm_info_dst['dst_input_path']}\\\\{all_PSRFM_sets[set_idx][instance-1]['fine_res_filenames'][0][0][:-4]}.hdr\")\n",
    "                        elif line.strip().startswith('NROWS'):\n",
    "                            print(f\"NROWS = {nrows}\")\n",
    "                        elif line.strip().startswith('COARSE_ROWS'):\n",
    "                            print(f\"COARSE_ROWS = {nrows}\")\n",
    "                        elif line.strip().startswith('NCOLS'):\n",
    "                            print(f\"NCOLS = {ncols}\")\n",
    "                        elif line.strip().startswith('COARSE_COLS'):\n",
    "                            print(f\"COARSE_COLS = {ncols}\")\n",
    "                        elif line.strip().startswith('NBANDS'):\n",
    "                            print(f\"NBANDS = {nbands}\")\n",
    "                        elif line.strip().startswith('RESOLUTION'):\n",
    "                            print(f\"RESOLUTION = {image_sets[satellite_choice]['pixel_size']}\")\n",
    "                        elif line.strip().startswith('BLOCK_SIZE'):\n",
    "                            print(f\"BLOCK_SIZE = {block_size}\")\n",
    "                        else:\n",
    "                            print(line, end = '')\n",
    "# fine_res_filenames_dates, mask_filenames_dates, coarse_res_filenames_dates, PSRFM_instances\n",
    "# coarse_res_filenames_dates, matching_date_indexes, PSRFM_sets, len(matching_date_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_file in [filename for filename in os.listdir(psrfm_info_dst['dst_param_path']) if filename != 'PSRFM_Main.exe']:\n",
    "    print(f'{psrfm_info_dst[\"dst_param_path\"]}\\\\{param_file}')\n",
    "    subprocess.run([f'{psrfm_info_dst[\"dst_param_path\"]}\\\\PSRFM_Main.exe', f'{psrfm_info_dst[\"dst_param_path\"]}\\\\{param_file}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting ENVI to tif, and adding csv files from headers for each image\n",
    "if not os.path.exists(psrfm_info_dst['dst_output_gtif_path']):\n",
    "    os.mkdir(psrfm_info_dst['dst_output_gtif_path'])\n",
    "\n",
    "instance_dirs = []\n",
    "for directoryname in os.listdir(psrfm_info_dst['dst_output_path']):\n",
    "    if \"instance\" in directoryname:\n",
    "        instance_dirs.append(directoryname)\n",
    "    \n",
    "for instance_dir in instance_dirs:\n",
    "    if not os.path.exists(f'{psrfm_info_dst[\"dst_output_gtif_path\"]}\\\\{instance_dir}'):\n",
    "        os.mkdir(f'{psrfm_info_dst[\"dst_output_gtif_path\"]}\\\\{instance_dir}')\n",
    "    for filename in [filename for filename in os.listdir(f'{psrfm_info_dst[\"dst_output_path\"]}\\\\{instance_dir}') \n",
    "                     if ('.dat' in filename and not '_mask' in filename and not '_Q' in filename)]:\n",
    "#       converting images to geotiff\n",
    "        with rio.open(f'{psrfm_info_dst[\"dst_output_path\"]}\\\\{instance_dir}\\\\{filename}') as image_to_convert:\n",
    "            \n",
    "            newargs = image_to_convert.meta.copy()\n",
    "            newargs.update({\n",
    "                'driver': 'GTiff'\n",
    "            })\n",
    "            \n",
    "            with rio.open(f'{psrfm_info_dst[\"dst_output_gtif_path\"]}\\\\{instance_dir}\\\\{filename[:-4]}.tif'.replace('-', '_'), 'w', **newargs) as dst:\n",
    "                dst.write(image_to_convert.read())\n",
    "#      generating metadata for geeup images\n",
    "#      getting data from PSRFM param files\n",
    "        col = filename.split('_')[0][-3]\n",
    "        row = filename.split('_')[0][-1]\n",
    "        reference_inst = ''\n",
    "        if 'reference' in instance_dir:\n",
    "            reference_inst = 'reference'\n",
    "        instance_number = instance_dir[-1]\n",
    "        param_filename = f'{col}_{row}_params{reference_inst}_{instance_number}.txt'\n",
    "        with open(f'{psrfm_info_dst[\"dst_param_path\"]}//{param_filename}', 'r') as param_file:\n",
    "            for line in param_file:\n",
    "                if line.strip().startswith('IN_PAIR_COARSE_FNAME'):\n",
    "                    ref_start_date = line.split('C:')[1].strip()[-15:-4]\n",
    "                    ref_end_date = line.split('C:')[2].strip()[-15:-4]\n",
    "                    break\n",
    "        prediction_date = datetime.datetime.strptime(\n",
    "            f'{filename[-21: -10]}T00:00:00Z', \"%d-%b-%YT%H:%M:%S%z\")\n",
    "        starttime = (prediction_date - datetime.datetime.strptime('1970-01-01T00:00:00Z', \n",
    "                                                                  \"%Y-%m-%dT%H:%M:%S%z\"))\\\n",
    "                    .total_seconds() * 1000\n",
    "        with open(f'{psrfm_info_dst[\"dst_output_path\"]}\\\\{instance_dir}\\\\{filename[:-4]}.hdr') as header_file:\n",
    "            with open(f'{psrfm_info_dst[\"dst_output_gtif_path\"]}\\\\{instance_dir}\\\\{filename[:-4]}.csv', 'w', newline='') as csv_file:\n",
    "                fieldnames = ['id_no', 'starttime', 'ref_start_date', 'ref_end_date']\n",
    "                writer = csv.DictWriter(csv_file, fieldnames)\n",
    "                writer.writerow({\n",
    "                    'id_no': f'{filename[:-4]}',\n",
    "                    'starttime': starttime,\n",
    "                    'ref_start_date': ref_start_date,\n",
    "                    'ref_end_date': ref_end_date,\n",
    "                })\n",
    "        # geeup permission denied :geeup upload --source C:\\Users\\karan\\Documents\\CCRS_2A_COOP\\PSRFM_Testing\\SentinelKelownaTest7\\output\\geotiffs\\instance_2\\testdir --dest users/ccrs1fy2021/geeuptest -m C:\\Users\\karan\\Documents\\CCRS_2A_COOP\\PSRFM_Testing\\SentinelKelownaTest7\\output\\geotiffs\\instance_2\\testdir\\metadata.csv -u hemit.shah@gcp.nrcan-rncan.cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
